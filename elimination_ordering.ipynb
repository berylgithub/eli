{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "from itertools import combinations\n",
    "from scipy.io import mmread, mminfo\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of Elimination Ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Normalize Stage'''\n",
    "'''preliminaries:\n",
    "- node == vertex (\"vertices\" for plural)\n",
    "- nodes' index start at 0\n",
    "- all graphs and trees are represented in matrix farm\n",
    "- valency is the sum of the edges' weights of a node, i.e. the sum of the row in a corresponding index, in numpy:\n",
    "    np.sum(graph[node_idx])\n",
    "- currently, the matrix is assumed to be symmetric (undirected)\n",
    "- a fill is only calculated during the elimination, not during the algorithm process to get the elimination ordering,\n",
    "and a fill is A[i][j] = 1 iff A[i][k] = A[k][i] = A[j][k] = A[k][j] = 1 then A[i][k] = A[k][i] = A[j][k] = A[k][j] = 0\n",
    "- the diagonal element is always ignored (must be zero'd first if it's nonzero)\n",
    "- non zeros' values doesn't matter (unweighted), it's always binary (0,1)\n",
    "'''\n",
    "'''independent functions:'''\n",
    "\n",
    "#for transforiming tree matrices to ordered list\n",
    "def topological_sort_tree(tree_in):\n",
    "    #look for the \"first\" node, which is the node with no incoming edges:\n",
    "    tree = np.copy(tree_in) #copy the tree so the input wont be affected\n",
    "    size = tree.shape[0]\n",
    "    S = []\n",
    "    for i in range(size):\n",
    "        #need to exclude disconnected nodes by checking row-wise too:\n",
    "        if np.sum(tree[:,i]) == 0:\n",
    "            if np.sum(tree[i]) > 0:\n",
    "                S.append(i)\n",
    "    #print(\"S\",S)\n",
    "    \n",
    "    enque = lambda q, x: q.append(x)\n",
    "    deque = lambda q: q.pop(0)\n",
    "    #kahn's algorithm for topological sort (https://en.wikipedia.org/wiki/Topological_sorting):\n",
    "    #input: tree, first_nodes\n",
    "    L = []\n",
    "    while len(S) > 0:\n",
    "        n = deque(S)\n",
    "        L.append(n)\n",
    "        ms = np.where(tree[n] == 1)[0]    #look for set of destination nodesf rom n (neighbours)\n",
    "        #for each node m with an edge e from n to m:\n",
    "        for m in ms:\n",
    "            tree[n][m] = 0 #remove edge e from the graph\n",
    "            if np.sum(tree[:,m]) == 0: #if m has no other incoming edges then\n",
    "                enque(S, m) #insert m into S\n",
    "    #there should be a final check whether the graph still has some edges, but it isnt necessary for tree cases since trees wont have DAG\n",
    "    return L\n",
    "\n",
    "#Breadth-First-Search traversal algorithm:\n",
    "def BFS(graph, source):\n",
    "    n_init = graph.shape[0]\n",
    "    q = []\n",
    "    enque = lambda q, x: q.append(x)\n",
    "    deque = lambda q: q.pop(0)\n",
    "    visited = np.array([0]*n_init)\n",
    "    distances = np.array([0]*n_init)\n",
    "    visited[source] = 1\n",
    "    enque(q, source)\n",
    "    q_counter = 1 #to keep track how many neighbours enqueued\n",
    "    path = []\n",
    "    while q:\n",
    "        v = deque(q)\n",
    "        q_counter = q_counter - 1\n",
    "        neighbours = np.where(graph[v] == 1)[0] #enque all v's neighbours (gamma(v))\n",
    "        for node_i in neighbours:\n",
    "            if (visited[node_i] == 0) and (node_i not in q):\n",
    "                enque(q, node_i)\n",
    "                visited[node_i] = 1\n",
    "                q_counter += 1\n",
    "        #print(v, neighbours, q, q_counter, depth)\n",
    "        path.append(v)\n",
    "    return path\n",
    "\n",
    "#get ordered list from merge forest\n",
    "def get_ordered_list_merged_vertex(tree, placed_vertex):\n",
    "    '''algorithm for tree-tracing that covers all scenarios:\n",
    "    0. transpose the tree (to get the reverse order), due to the nature of the merge procedure, the leaves will be the roots\n",
    "    1. topological sort to get the root(s)\n",
    "    2. determine the roots by checking the connections between vertices\n",
    "    3. if there are more than one roots:\n",
    "        BFS traverse starting from the placed node to get the ordered lists\n",
    "    else:\n",
    "        just use the list from the topological sort as the ordered list\n",
    "    '''\n",
    "    #print(\"edges:\", np.where(tree.T == 1))\n",
    "    topological_list = topological_sort_tree(tree.T)\n",
    "    #print(\"topological_list\",topological_list)\n",
    "    '''\n",
    "    print(\"topological_list\",topological_list)\n",
    "    print(tree)\n",
    "    print(tree.T)\n",
    "    '''\n",
    "    #check the number of roots and get the corresponding roots:\n",
    "    length = len(topological_list)\n",
    "    roots = [topological_list[0]]\n",
    "    for i_elem in topological_list:\n",
    "        non_root_found = False\n",
    "        for j_elem in topological_list:\n",
    "            if i_elem != j_elem:\n",
    "                if tree.T[i_elem][j_elem] == 0:\n",
    "                    #print(i_elem, j_elem)\n",
    "                    roots.append(j_elem)\n",
    "                else:\n",
    "                    non_root_found = True\n",
    "                    break\n",
    "        if non_root_found:\n",
    "            break\n",
    "    #print(\"roots\",roots)\n",
    "    #if more than one roots, do BFS starting from the placed node, else just use the topological_list:\n",
    "    ordered_list = None\n",
    "    #print(\"ordlist bfs reversed:\",list(reversed(BFS(tree, placed_vertex))))\n",
    "    if len(roots) > 1:\n",
    "        #ordered_list = BFS(tree.T, placed_vertex)\n",
    "        ordered_list = list(reversed(BFS(tree, placed_vertex)))\n",
    "        #print(\"orderedlist bfs\",ordered_list)\n",
    "    else:\n",
    "        ordered_list = topological_list\n",
    "        #print(\"orderedlist\",topological_list)\n",
    "    #print(\"ordered_list\",ordered_list)\n",
    "    return ordered_list\n",
    "\n",
    "#clique checker:\n",
    "def clique_check(graph, vertices_idx):\n",
    "    #get subgraph, by slicing indexes:\n",
    "    subgraph = graph[vertices_idx][:,vertices_idx]\n",
    "    n = subgraph.shape[0]\n",
    "    #check for clique from subgraph:\n",
    "    upper_tri = subgraph[np.triu_indices(n, 1)]\n",
    "    return np.sum(upper_tri) == comb(n, 2)\n",
    "\n",
    "#subset checker:\n",
    "def check_subset(graph, neighbours):\n",
    "    bool_subset = False\n",
    "    j_get = None\n",
    "    for j_node in neighbours:\n",
    "        #probably need to be stopped earlier? instead of taking the last neighbour index\n",
    "        gamma_j = np.where(graph[j_node] == 1)[0]\n",
    "        j_T = np.append(gamma_j, j_node) #j^up_tack = j union gamma(j):= j added to its neighbours\n",
    "        if set(neighbours).issubset(set(j_T)): #gamma(i) \\subset j^up_tack\n",
    "            bool_subset = True\n",
    "            j_get = j_node\n",
    "            break #stop when found\n",
    "    return bool_subset, j_get\n",
    "\n",
    "#more accurate way of checking the total nodes within a graph, since the edge is represented \n",
    "#by the value of A[i][j] cell, e.g if i <-> j is connected, it means A[i][j] = A[j][i] 1, otherwise 0, \n",
    "#so the size of the matrix may not correspond to the total number of nodes\n",
    "def get_total_nodes(graph, row_size):\n",
    "    counter = 0\n",
    "    for i in range(row_size):\n",
    "        if np.sum(graph[i]) > 0:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "'''end'''\n",
    "\n",
    "\n",
    "def initialize(graph):\n",
    "    '''function for data initialization, returns:\n",
    "    - e vector placeholder\n",
    "    - weight vector w\n",
    "    - empty merge forest\n",
    "    - first zero and last zero idxes\n",
    "    - deleted bool array\n",
    "    '''\n",
    "    n = graph.shape[0]\n",
    "    e = np.array([-1]*n) #for now the placeholder is an array of -1\n",
    "    w = np.array([1]*n) #weight vector for merge forest\n",
    "    merge_forest = np.zeros((n,n)) #merge forest for assessment criteria\n",
    "    deleted = np.array([False]*n)\n",
    "    first_zero = 0; last_zero = -1\n",
    "    return e, w, first_zero, last_zero, merge_forest, deleted\n",
    "\n",
    "#normalize stage:\n",
    "def normalize(graph):\n",
    "    global deleted, e, w, first_zero, last_zero, merge_forest\n",
    "    n = n_init = graph.shape[0] #number of nodes\n",
    "    '''e = np.array([-1]*n) #for now the placeholder is an array of -1\n",
    "    w = np.array([1]*n) #weight vector for merge forest\n",
    "    merge_forest = np.zeros((n,n)) #merge forest for assessment criteria'''\n",
    "    modified = np.array([1]*n) #modified = 1, otherwise 0'''\n",
    "\n",
    "\n",
    "    #normalize stage\n",
    "    #for now, cyclic ordering assumption: start from the 1st index to last idx, hence for-loop\n",
    "    #need merge check for every node passed, by w[i] > 1\n",
    "    #print(\"i, n, m, valency, e, summodified, firstzero, lastzero\")\n",
    "    while np.sum(modified) > 0:\n",
    "        #print()\n",
    "        #for i in range(n_init):\n",
    "        for i in range(n_init):\n",
    "            #check if it is already deleted, if yes, skip:\n",
    "            if deleted[i]: #deleted in prev round\n",
    "                modified[i] = 0 #set modified to 0\n",
    "                #print(\"already deleted:\",i)\n",
    "                continue\n",
    "            if np.sum(modified) == 0:\n",
    "                break\n",
    "            #recalculate all of the values:\n",
    "            n = get_total_nodes(graph, n_init) #recalculate n by excluding zero vectored rows (disconnected vertices)\n",
    "            valencies = np.array([np.sum(graph[j]) for j in range(n_init)]) #needs to recalculate the valency for each update due to the graph-change\n",
    "            #print(i,n,m,valency,valencies,e,modified)\n",
    "            mean_valency = np.sum(valencies)/n #get mean valency\n",
    "            max_valency = np.max(valencies) #get max valency\n",
    "            valency = np.sum(graph[i]) #get vertex's valency\n",
    "            m = np.min([mean_valency, np.floor(n**(1/4) + 3)])\n",
    "            #m = np.floor(n**(1/4) + 3) #probably this is the correct interpretiation\n",
    "            #print(\"mean_valency, np.floor(n**(1/4) + 3)\",mean_valency, np.floor(n**(1/4) + 3))\n",
    "            neighbours = np.where(graph[i] == 1)[0] #get the neighbours of i\n",
    "            #print(i,n,m,valency,e,np.sum(modified),first_zero, last_zero)\n",
    "            #print(\"neighbours\",neighbours)\n",
    "            #check all of the conditions based on the valency\n",
    "            if valency == n-1:\n",
    "                ##always check for merge - i.e w[i] > 1\n",
    "                if w[i] > 1:\n",
    "                    ordered_list = get_ordered_list_merged_vertex(merge_forest, i)\n",
    "                    len_e = len(e)\n",
    "                    len_ord_list = len(ordered_list)\n",
    "                    e[len_e + last_zero - len_ord_list + 1 : len_e + last_zero + 1] = ordered_list #lastzero placement\n",
    "                    last_zero -= len_ord_list #decrement last zero by the size of the ordered list \n",
    "                else:\n",
    "                    #add to the last zero and decrement the indexer:\n",
    "                    e[last_zero] = i\n",
    "                    last_zero -= 1\n",
    "                graph[i] = graph[:,i] = 0  #remove from graph by deleting edges\n",
    "                deleted[i] = True\n",
    "                #graph = np.delete(graph, i, 0) #delete from graph -- this should be the proper deletion method, though not sure if it's faster\n",
    "                #graph = np.delete(graph, i, 1)\n",
    "                modified[neighbours] = 1 #set neighbours as modified\n",
    "                #print(\"rule 1\")\n",
    "            elif (valency > np.ceil(n/2)) and (valency == max_valency):\n",
    "                if w[i] > 1:\n",
    "                    ordered_list = get_ordered_list_merged_vertex(merge_forest, i)\n",
    "                    len_e = len(e)\n",
    "                    len_ord_list = len(ordered_list)\n",
    "                    e[len_e + last_zero - len_ord_list + 1 : len_e + last_zero + 1] = ordered_list\n",
    "                    last_zero -= len_ord_list\n",
    "                else:\n",
    "                    e[last_zero] = i\n",
    "                    last_zero -= 1\n",
    "                graph[i] = graph[:,i] = 0\n",
    "                deleted[i] = True\n",
    "                modified[neighbours] = 1\n",
    "                #print(\"rule 2\")\n",
    "            elif valency <= 1:\n",
    "                #e.insert(0, i) #place vertex first\n",
    "                if w[i] > 1:\n",
    "                    ordered_list = get_ordered_list_merged_vertex(merge_forest, i)\n",
    "                    len_e = len(e)\n",
    "                    len_ord_list = len(ordered_list)\n",
    "                    e[first_zero : first_zero + len_ord_list] = ordered_list #insert by firstzero pos\n",
    "                    first_zero += len_ord_list #increment the first zero by the size of the ordered list\n",
    "                else:\n",
    "                    #add to the first zero pos and increment the indexer:\n",
    "                    e[first_zero] = i\n",
    "                    first_zero += 1\n",
    "                graph[i] = graph[:,i] = 0\n",
    "                deleted[i] = True\n",
    "                modified[neighbours] = 1\n",
    "                #print(\"rule 3\")\n",
    "            elif valency == 2:\n",
    "                #e.insert(0, i)\n",
    "                if w[i] > 1:\n",
    "                    ordered_list = get_ordered_list_merged_vertex(merge_forest, i)\n",
    "                    len_e = len(e)\n",
    "                    len_ord_list = len(ordered_list)\n",
    "                    e[first_zero : first_zero + len_ord_list] = ordered_list #insert by firstzero pos\n",
    "                    first_zero += len_ord_list\n",
    "                else:\n",
    "                    #add to the first zero pos and increment the indexer:\n",
    "                    e[first_zero] = i\n",
    "                    first_zero += 1\n",
    "                graph[neighbours[0]][neighbours[1]] = graph[neighbours[1]][neighbours[0]] = 1 #make edge between them -- fill the value of the cell with 1\n",
    "                graph[i] = graph[:,i] = 0\n",
    "                deleted[i] = True\n",
    "                modified[neighbours] = 1\n",
    "                #print(\"rule 4\")\n",
    "            elif (valency <= m) and (clique_check(graph, neighbours)):\n",
    "                #e.insert(0, i)\n",
    "                if w[i] > 1:\n",
    "                    ordered_list = get_ordered_list_merged_vertex(merge_forest, i)\n",
    "                    #print(\"tree[i]\",np.where(merge_forest[i] == 1))\n",
    "                    len_e = len(e)\n",
    "                    len_ord_list = len(ordered_list)\n",
    "                    e[first_zero : first_zero + len_ord_list] = ordered_list #insert by firstzero pos\n",
    "                    first_zero += len_ord_list\n",
    "                    #print(\"place multiple nodes\",ordered_list)\n",
    "                else:\n",
    "                    #add to the first zero pos and increment the indexer:\n",
    "                    #print(\"place one node\")\n",
    "                    e[first_zero] = i\n",
    "                    first_zero += 1\n",
    "                graph[i] = graph[:,i] = 0\n",
    "                deleted[i] = True\n",
    "                modified[neighbours] = 1\n",
    "                #print(\"rule 5\")\n",
    "            elif (valency <= m): \n",
    "                bool_subset, j_node = check_subset(graph, neighbours) #gamma(i) \\subset j^uptack, j \\in gamma(i)\n",
    "                if bool_subset:\n",
    "                    merge_forest[j_node][i] = 1 #merge i into j, add directed edge j->i\n",
    "                    w[j_node] += 1 #increment weight\n",
    "                    graph[i] = graph[:,i] = 0\n",
    "                    deleted[i] = True\n",
    "                    modified[neighbours] = 1\n",
    "                    #print(\"rule 6\")\n",
    "                    #print(neighbours, modified[neighbours])\n",
    "                    #print(\"merged\",i,j_node)\n",
    "            modified[i] = 0 #set vertex as unmodified\n",
    "            #print(\"w,deleted\",w[i],np.where(deleted == True)[0])\n",
    "            #print()\n",
    "    #return e, w, first_zero, last_zero, merge_forest\n",
    "    #return first_zero, last_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S [2, 3]\n",
      "[ 7  9  8 -1 -1 -1 -1 -1  5  6] [3, 1]\n",
      "10 2\n",
      "5 8\n",
      "[ 7  9  8 -1 -1 -1  3  1  5  6]\n"
     ]
    }
   ],
   "source": [
    "'''merge-forest placement test'''\n",
    "dummy_order = np.array([7,9,8,-1,-1,-1,-1,-1,5,6])\n",
    "dummy_placed = 3\n",
    "dum_fz = 3\n",
    "dum_lz = -3\n",
    "\n",
    "#topological sort:\n",
    "t = np.array([[0,0,1,0,0], #0-2,1-3,4-2\n",
    "              [0,0,0,1,0],\n",
    "              [0,0,0,0,0],\n",
    "              [0,0,0,0,0],\n",
    "              [0,0,1,0,0]\n",
    "             ])\n",
    "'''\n",
    "t = np.array([[0,1,0,0],\n",
    "              [0,0,1,0],\n",
    "              [0,0,0,1],\n",
    "              [0,0,0,0]])\n",
    "'''\n",
    "\n",
    "\n",
    "ordered_list = get_ordered_list_merged_vertex(t, dummy_placed)\n",
    "print(dummy_order,ordered_list)\n",
    "#place first:\n",
    "length = len(ordered_list)\n",
    "length_do = len(dummy_order)\n",
    "print(length_do, length)\n",
    "dummy_order[dum_fz : dum_fz + length] = ordered_list #firstzero placement\n",
    "print(length_do + dum_lz - length, length_do + dum_lz + 1) #5,8\n",
    "dummy_order[length_do + dum_lz - length + 1: length_do + dum_lz + 1] = ordered_list #lastzero placement\n",
    "print(dummy_order)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Graphs used for initial assesment:\n",
    "graph is:   \n",
    "     4--6--5\n",
    "     |  | /\n",
    "  3--7--1\n",
    "  |  |  |\n",
    "  8--0--2\n",
    "  \n",
    "graph_1 is a bipartite graph (Nauru graph): https://en.wikipedia.org/wiki/Adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 1 1]\n",
      " [0 0 1 0 0 1 1 1 0]\n",
      " [1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 1 1 0]\n",
      " [0 1 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 1 1 0 0 0]\n",
      " [1 1 0 1 1 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0]]\n",
      "True\n",
      "i, n, m, valency, valencies, e, modified, firstzero, lastzero\n",
      "\n",
      "0 9 2.6666666666666665 3 [3 4 2 2 2 2 3 4 2] [-1 -1 -1 -1 -1 -1 -1 -1 -1] [1 1 1 1 1 1 1 1 1] 9 0 -1\n",
      "neighbours [2 7 8]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "1 9 2.6666666666666665 4 [3 4 2 2 2 2 3 4 2] [-1 -1 -1 -1 -1 -1 -1 -1 -1] [0 1 1 1 1 1 1 1 1] 8 0 -1\n",
      "neighbours [2 5 6 7]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "2 9 2.6666666666666665 2 [3 4 2 2 2 2 3 4 2] [-1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 1 1 1 1 1 1 1] 7 0 -1\n",
      "neighbours [0 1]\n",
      "rule 4\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] [2]\n",
      "\n",
      "3 8 2.75 2 [3 4 0 2 2 2 3 4 2] [ 2 -1 -1 -1 -1 -1 -1 -1 -1] [1 1 0 1 1 1 1 1 1] 8 1 -1\n",
      "neighbours [7 8]\n",
      "rule 4\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] [2 3]\n",
      "\n",
      "4 7 2.857142857142857 2 [3 4 0 0 2 2 3 4 2] [ 2  3 -1 -1 -1 -1 -1 -1 -1] [1 1 0 0 1 1 1 1 1] 7 2 -1\n",
      "neighbours [6 7]\n",
      "rule 4\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] [2 3 4]\n",
      "\n",
      "5 6 3.0 2 [3 4 0 0 0 2 3 4 2] [ 2  3  4 -1 -1 -1 -1 -1 -1] [1 1 0 0 0 1 1 1 1] 6 3 -1\n",
      "neighbours [1 6]\n",
      "rule 4\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] [2 3 4 5]\n",
      "\n",
      "6 5 2.8 2 [3 3 0 0 0 0 2 4 2] [ 2  3  4  5 -1 -1 -1 -1 -1] [1 1 0 0 0 0 1 1 1] 5 4 -1\n",
      "neighbours [1 7]\n",
      "rule 4\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] [2 3 4 5 6]\n",
      "\n",
      "7 4 2.5 3 [3 2 0 0 0 0 0 3 2] [ 2  3  4  5  6 -1 -1 -1 -1] [1 1 0 0 0 0 0 1 1] 4 5 -1\n",
      "neighbours [0 1 8]\n",
      "rule 1\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] [2 3 4 5 6 7]\n",
      "\n",
      "8 3 1.3333333333333333 1 [2 1 0 0 0 0 0 0 1] [ 2  3  4  5  6 -1 -1 -1  7] [1 1 0 0 0 0 0 0 1] 3 5 -2\n",
      "neighbours [0]\n",
      "rule 3\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] [2 3 4 5 6 7 8]\n",
      "\n",
      "\n",
      "0 2 1.0 1 [1 1 0 0 0 0 0 0 0] [ 2  3  4  5  6  8 -1 -1  7] [1 1 0 0 0 0 0 0 0] 2 6 -2\n",
      "neighbours [1]\n",
      "rule 1\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] [0 2 3 4 5 6 7 8]\n",
      "\n",
      "1 0 nan 0 [0 0 0 0 0 0 0 0 0] [ 2  3  4  5  6  8 -1  0  7] [0 1 0 0 0 0 0 0 0] 1 6 -3\n",
      "neighbours []\n",
      "rule 3\n",
      "w,deleted [1 1 1 1 1 1 1 1 1] [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "already deleted: 2\n",
      "already deleted: 3\n",
      "already deleted: 4\n",
      "already deleted: 5\n",
      "already deleted: 6\n",
      "already deleted: 7\n",
      "already deleted: 8\n",
      "None\n",
      "e, w, first_zero, last_zero, merge_forest, deleted [2 3 4 5 6 8 1 0 7] [1 1 1 1 1 1 1 1 1] 7 -3 [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]] [ True  True  True  True  True  True  True  True  True]\n",
      "i, n, m, valency, valencies, e, modified, firstzero, lastzero\n",
      "\n",
      "0 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 24 0 -1\n",
      "neighbours [ 1  5 21]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "1 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 23 0 -1\n",
      "neighbours [ 0  3 15]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "2 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 22 0 -1\n",
      "neighbours [ 3  4 23]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "3 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 21 0 -1\n",
      "neighbours [1 2 9]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "4 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 20 0 -1\n",
      "neighbours [ 2  5 17]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "5 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 19 0 -1\n",
      "neighbours [ 0  4 11]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "6 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 18 0 -1\n",
      "neighbours [ 7 11 19]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "7 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 17 0 -1\n",
      "neighbours [ 6  9 13]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "8 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 16 0 -1\n",
      "neighbours [ 9 10 22]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "9 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 15 0 -1\n",
      "neighbours [3 7 8]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "10 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 14 0 -1\n",
      "neighbours [ 8 11 16]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "11 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1] 13 0 -1\n",
      "neighbours [ 5  6 10]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "12 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1] 12 0 -1\n",
      "neighbours [13 17 18]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "13 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1] 11 0 -1\n",
      "neighbours [ 7 12 15]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "14 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1] 10 0 -1\n",
      "neighbours [15 16 20]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "15 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1] 9 0 -1\n",
      "neighbours [ 1 13 14]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "16 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1] 8 0 -1\n",
      "neighbours [10 14 17]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "17 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1] 7 0 -1\n",
      "neighbours [ 4 12 16]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "18 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1] 6 0 -1\n",
      "neighbours [12 19 23]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "19 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1] 5 0 -1\n",
      "neighbours [ 6 18 21]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "20 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1] 4 0 -1\n",
      "neighbours [14 21 22]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "21 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1] 3 0 -1\n",
      "neighbours [ 0 19 20]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "22 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1] 2 0 -1\n",
      "neighbours [ 8 20 23]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "23 24 3.0 3.0 [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.] [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1] 1 0 -1\n",
      "neighbours [ 2 18 22]\n",
      "w,deleted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] []\n",
      "\n",
      "None\n",
      "e, w, first_zero, last_zero, merge_forest, deleted [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 0 -1 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beryl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:195: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "'''graphs collection for normalization testing'''\n",
    "#test array/graph from https://people.sc.fsu.edu/~jburkardt/m_src/rcm/rcm.html\n",
    "graph = np.array([\n",
    "            [0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 1, 0, 0, 1, 1, 1, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "            [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
    "            [1, 1, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "             ])\n",
    "print(graph)\n",
    "\n",
    "#nauru graph\n",
    "graph_1 = np.zeros((24,24))\n",
    "graph_1[0][1] = graph_1[0][5] = graph_1[0][21] = \\\n",
    "graph_1[1][3] = graph_1[1][15] = \\\n",
    "graph_1[2][3] = graph_1[2][4] = graph_1[2][23] = \\\n",
    "graph_1[3][9] = graph_1[4][5] = graph_1[4][17] = \\\n",
    "graph_1[5][11] = graph_1[6][7] = graph_1[6][11] = graph_1[6][19] = \\\n",
    "graph_1[7][9] = graph_1[7][13] = \\\n",
    "graph_1[8][9] = graph_1[8][10] = graph_1[8][22] = \\\n",
    "graph_1[10][11] = graph_1[10][16] = \\\n",
    "graph_1[12][13] = graph_1[12][17] = graph_1[12][18] = \\\n",
    "graph_1[13][15] = graph_1[14][15] = graph_1[14][16] = graph_1[14][20] = \\\n",
    "graph_1[16][17] = graph_1[18][19] = graph_1[18][23] = \\\n",
    "graph_1[19][21] = graph_1[20][21] = graph_1[20][22] = \\\n",
    "graph_1[22][23] = 1\n",
    "graph_1 += graph_1.T\n",
    "print(np.allclose(graph_1, graph_1.T, rtol=1e-05, atol=1e-08))\n",
    "\n",
    "'''testing-ground for normalize stage'''\n",
    "e, w, first_zero, last_zero, merge_forest, deleted = initialize(graph)\n",
    "print(normalize(graph))\n",
    "print(\"e, w, first_zero, last_zero, merge_forest, deleted\",e, w, first_zero, last_zero, merge_forest, deleted)\n",
    "\n",
    "e, w, first_zero, last_zero, merge_forest, deleted = initialize(graph_1)\n",
    "print(normalize(graph_1))\n",
    "print(\"e, w, first_zero, last_zero, merge_forest, deleted\",e, w, first_zero, last_zero, merge_forest, deleted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g2:\n",
      "symm True\n",
      "clique True\n",
      "1^up_tack (True, 1)\n",
      "i, n, m, valency, valencies, e, modified, firstzero, lastzero\n",
      "\n",
      "4.285714285714286 4.0\n",
      "0 7 4.0 3 [3 4 5 6 5 4 3] [-1 -1 -1 -1 -1 -1 -1] [1 1 1 1 1 1 1] 7 0 -1\n",
      "rule 5\n",
      "w [1 1 1 1 1 1 1]\n",
      "\n",
      "4.0 4.0\n",
      "1 6 4.0 3 [0 3 4 5 5 4 3] [ 0 -1 -1 -1 -1 -1 -1] [0 1 1 1 1 1 1] 6 1 -1\n",
      "rule 5\n",
      "w [1 1 1 1 1 1 1]\n",
      "\n",
      "3.6 4.0\n",
      "2 5 3.6 3 [0 0 3 4 4 4 3] [ 0  1 -1 -1 -1 -1 -1] [0 0 1 1 1 1 1] 5 2 -1\n",
      "rule 5\n",
      "w [1 1 1 1 1 1 1]\n",
      "\n",
      "3.0 4.0\n",
      "3 4 3.0 3 [0 0 0 3 3 3 3] [ 0  1  2 -1 -1 -1 -1] [0 0 0 1 1 1 1] 4 3 -1\n",
      "rule 1\n",
      "w [1 1 1 1 1 1 1]\n",
      "\n",
      "2.0 4.0\n",
      "4 3 2.0 2 [0 0 0 0 2 2 2] [ 0  1  2 -1 -1 -1  3] [0 0 0 0 1 1 1] 3 3 -2\n",
      "rule 1\n",
      "w [1 1 1 1 1 1 1]\n",
      "\n",
      "1.0 4.0\n",
      "5 2 1.0 1 [0 0 0 0 0 1 1] [ 0  1  2 -1 -1  4  3] [0 0 0 0 0 1 1] 2 3 -3\n",
      "rule 1\n",
      "w [1 1 1 1 1 1 1]\n",
      "\n",
      "nan 3.0\n",
      "6 0 nan 0 [0 0 0 0 0 0 0] [ 0  1  2 -1  5  4  3] [0 0 0 0 0 0 1] 1 3 -4\n",
      "rule 3\n",
      "w [1 1 1 1 1 1 1]\n",
      "\n",
      "[0 1 2 6 5 4 3]\n",
      "g3:\n",
      "clique False\n",
      "1^up_tack (True, 2)\n",
      "g4:\n",
      "[[0 1 1 1 1 0 0 0]\n",
      " [1 0 1 1 1 1 0 0]\n",
      " [1 1 0 1 1 1 1 0]\n",
      " [1 1 1 0 1 1 1 1]\n",
      " [1 1 1 1 0 1 1 1]\n",
      " [0 1 1 1 1 0 1 1]\n",
      " [0 0 1 1 1 1 0 1]\n",
      " [0 0 0 1 1 1 1 0]]\n",
      "clique True\n",
      "1^up_tack (True, 1)\n",
      "i, n, m, valency, valencies, e, modified, firstzero, lastzero\n",
      "\n",
      "5.5 4.0\n",
      "0 8 4.0 4 [4 5 6 7 7 6 5 4] [-1 -1 -1 -1 -1 -1 -1 -1] [1 1 1 1 1 1 1 1] 8 0 -1\n",
      "rule 5\n",
      "w [1 1 1 1 1 1 1 1]\n",
      "\n",
      "5.142857142857143 4.0\n",
      "1 7 4.0 4 [0 4 5 6 6 6 5 4] [ 0 -1 -1 -1 -1 -1 -1 -1] [0 1 1 1 1 1 1 1] 7 1 -1\n",
      "rule 5\n",
      "w [1 1 1 1 1 1 1 1]\n",
      "\n",
      "4.666666666666667 4.0\n",
      "2 6 4.0 4 [0 0 4 5 5 5 5 4] [ 0  1 -1 -1 -1 -1 -1 -1] [0 0 1 1 1 1 1 1] 6 2 -1\n",
      "rule 5\n",
      "w [1 1 1 1 1 1 1 1]\n",
      "\n",
      "4.0 4.0\n",
      "3 5 4.0 4 [0 0 0 4 4 4 4 4] [ 0  1  2 -1 -1 -1 -1 -1] [0 0 0 1 1 1 1 1] 5 3 -1\n",
      "rule 1\n",
      "w [1 1 1 1 1 1 1 1]\n",
      "\n",
      "3.0 4.0\n",
      "4 4 3.0 3 [0 0 0 0 3 3 3 3] [ 0  1  2 -1 -1 -1 -1  3] [0 0 0 0 1 1 1 1] 4 3 -2\n",
      "rule 1\n",
      "w [1 1 1 1 1 1 1 1]\n",
      "\n",
      "2.0 4.0\n",
      "5 3 2.0 2 [0 0 0 0 0 2 2 2] [ 0  1  2 -1 -1 -1  4  3] [0 0 0 0 0 1 1 1] 3 3 -3\n",
      "rule 1\n",
      "w [1 1 1 1 1 1 1 1]\n",
      "\n",
      "1.0 4.0\n",
      "6 2 1.0 1 [0 0 0 0 0 0 1 1] [ 0  1  2 -1 -1  5  4  3] [0 0 0 0 0 0 1 1] 2 3 -4\n",
      "rule 1\n",
      "w [1 1 1 1 1 1 1 1]\n",
      "\n",
      "nan 3.0\n",
      "7 0 nan 0 [0 0 0 0 0 0 0 0] [ 0  1  2 -1  6  5  4  3] [0 0 0 0 0 0 0 1] 1 3 -5\n",
      "rule 3\n",
      "w [1 1 1 1 1 1 1 1]\n",
      "\n",
      "[0 1 2 7 6 5 4 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beryl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:189: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "\"\"\"testing the merge rule (r6) using banded matrix\"\"\"\n",
    "graph_2 = np.array([[0,1,1,1,0,0,0],\n",
    "                    [1,0,1,1,1,0,0],\n",
    "                    [1,1,0,1,1,1,0],\n",
    "                    [1,1,1,0,1,1,1],\n",
    "                    [0,1,1,1,0,1,1],\n",
    "                    [0,0,1,1,1,0,1],\n",
    "                    [0,0,0,1,1,1,0]\n",
    "                   ])\n",
    "print(\"g2:\")\n",
    "print(\"symm\", np.allclose(graph_2, graph_2.T, rtol=1e-05, atol=1e-08))\n",
    "print(\"clique\", clique_check(graph_2, [1,2,3]))\n",
    "print(\"1^up_tack\", check_subset(graph_2, [1,2,3]))\n",
    "e, w, first_zero, last_zero, merge_forest = initialize(graph_2)\n",
    "print(normalize(graph_2, e, w, first_zero, last_zero, merge_forest))\n",
    "\n",
    "\n",
    "graph_3 = np.array([[0,1,1,1],\n",
    "                    [1,0,1,0],\n",
    "                    [1,1,0,1],\n",
    "                    [1,0,1,0]])\n",
    "print(\"g3:\")\n",
    "print(\"clique\", clique_check(graph_3, [1,2,3]))\n",
    "print(\"1^up_tack\", check_subset(graph_3, [1,2,3]))\n",
    "\n",
    "#test using another bamded matrix\n",
    "from scipy.sparse import diags\n",
    "graph_4 = diags([1,1,1, 1, 0, 1, 1,1,1], [-4,-3,-2,-1, 0, 1,2,3,4], shape=(8, 8), dtype=int).toarray()\n",
    "print(\"g4:\")\n",
    "print(graph_4)\n",
    "print(\"clique\", clique_check(graph_4, [1,2,3]))\n",
    "print(\"1^up_tack\", check_subset(graph_4, [1,2,3]))\n",
    "e, w, first_zero, last_zero, merge_forest = initialize(graph_4)\n",
    "print(normalize(graph_4, e, w, first_zero, last_zero, merge_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separation Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e,w,first_zero, last_zero, merge_forest, deleted: \n",
      " [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 18 10  1  3] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 0 -5 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] (array([ 1,  3, 10, 18], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "'''dijkstra https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm''' \n",
    "#function to help djikstra algorithm:\n",
    "def get_min_distance_vertex(Q, distances):\n",
    "    min_dist = float(\"inf\")\n",
    "    min_v = None\n",
    "    for v in range(Q.shape[0]):\n",
    "        if (distances[v] < min_dist) and (Q[v] == 1):\n",
    "            min_dist = distances[v]\n",
    "            min_v = v\n",
    "    return min_dist, min_v\n",
    "\n",
    "#start of dijkstra algorithm:\n",
    "def dijkstra_shortest_path(graph, source):\n",
    "    #n_init = get_total_nodes(graph, graph.shape[0])\n",
    "    n_init = graph.shape[0]\n",
    "    Q = np.array([1]*n_init)\n",
    "    #print(Q)\n",
    "    #source = root = 0\n",
    "    distances = np.array([float(\"inf\")]*n_init) #set distance vector\n",
    "    distances[source] = 0\n",
    "    prev = np.array([None]*n_init)\n",
    "\n",
    "    while np.sum(Q) > 0:\n",
    "        _, u = get_min_distance_vertex(Q, distances) #get the vertex with minimum distance\n",
    "        Q[u] = False #remove u from Q\n",
    "        neighbours = np.where(graph[u] == 1)[0]\n",
    "#        print(\"len(Q), neighbours\",len(Q), neighbours)\n",
    "        for v in neighbours:\n",
    "            if Q[v] == 1:\n",
    "                alt = distances[u] + graph[u][v] #distance is equal to the weight of the edge between u and v\n",
    "                if alt < distances[v]:\n",
    "                    distances[v] = alt\n",
    "                    prev[v] = u\n",
    "                #print(alt)\n",
    "    return distances, prev\n",
    "\n",
    "#function to find max valency from nodes\n",
    "def get_max_valency(subset_nodes, valencies):\n",
    "    max_valency = -float(\"inf\")\n",
    "    max_vertex = None\n",
    "    for m in subset_nodes:\n",
    "        if valencies[m] > max_valency:\n",
    "            max_valency = valencies[m]\n",
    "            max_vertex = m\n",
    "    return max_vertex, max_valency\n",
    "\n",
    "'''Separate Stage'''\n",
    "def separate(graph):\n",
    "    global deleted, e, w, first_zero, last_zero, merge_forest\n",
    "    n_init = graph.shape[0] #actual graph size\n",
    "    \n",
    "    '''RCM part'''\n",
    "    #1, d=0, pick vertex e with max valency:\n",
    "    #print(\"#1: \")\n",
    "    d_prime = 0\n",
    "    n_nodes = get_total_nodes(graph, graph.shape[0]) #current total nodes\n",
    "    valencies = np.array([np.sum(graph[i]) for i in range(n_init)])\n",
    "    e_sep = np.argmax(valencies) #get the node with max valency\n",
    "\n",
    "    #2, need to find a set of M with max distansce from e, which requires BFS or djikstra:\n",
    "    #print(\"#2: \")\n",
    "    distances, _ = dijkstra_shortest_path(graph, e_sep)\n",
    "    #print(\"distances\",distances)\n",
    "    conn_components = np.where(distances != np.inf)[0] #indexes of connected components within the subgraph where e resides\n",
    "    conn_distances = distances[conn_components] #distances of connected components (distances excluding inf)\n",
    "    s = conn_components.shape[0] #total connected components\n",
    "    d = np.max(conn_distances) #max distance from e\n",
    "    M = np.where(distances == d)[0] #set of vertices with max distance from e\n",
    "    #print(\"n_init, valencies, e_sep, s, d, M, conn_distances\")\n",
    "    #print(n_init, valencies, e_sep, s, d, M, conn_distances)\n",
    "    \n",
    "    \n",
    "    #3, if d'>d, d'=d, pick a vertex from M with max valency, back to 2 if the first e is close to the second e\n",
    "    #print(\"#3: \")\n",
    "    loopcount = 0 #for repetition statistics\n",
    "    while d>d_prime:\n",
    "        #print(\"d, d_prime, e_sep\",d, d_prime, e_sep)\n",
    "        d_prime = d\n",
    "        max_vertex,_ = get_max_valency(M, valencies)\n",
    "        #print(\"M, valencies\",M, valencies)\n",
    "        e_sep = max_vertex\n",
    "\n",
    "        #do 2 again:\n",
    "        distances, _ = dijkstra_shortest_path(graph, e_sep)\n",
    "        conn_components = np.where(distances != np.inf)[0] #indexes of connected components within the subgraph where e resides\n",
    "        conn_distances = distances[conn_components] #distances of connected components (distances excluding inf)\n",
    "        s = conn_components.shape[0] #total connected components\n",
    "        d = np.max(conn_distances) #max distance from e\n",
    "        M = np.where(distances == d)[0] #set of vertices with max distance from e\n",
    "        loopcount+=1\n",
    "    #print(\"RCM loopcount\", loopcount)\n",
    "    #print(\"n_init, valencies, e_sep, s, d, M, conn_distances\")\n",
    "    #print(n_init, valencies, e_sep, s, d, M, conn_distances)\n",
    "    '''end of RCM'''\n",
    "\n",
    "    \n",
    "    #3.5, get the n_k from e, 0<=k<=d, d=max distance, k \\in Z\n",
    "    #print(\"#3.5: n_k from e, 0<=k<=d, d=max distance\")\n",
    "    max_d = np.max(conn_distances).astype(int)\n",
    "    n_arr = np.zeros(max_d+1)\n",
    "    for i in range(0,max_d+1):\n",
    "        n_arr[i] = np.where(conn_distances == i)[0].shape[0]\n",
    "    #print(\"n_arr\",n_arr)\n",
    "    \n",
    "    \n",
    "    #4, initialization of several variables:\n",
    "    ##NOTE: there are two n's, n_k and n_{k+1}, which will be used for comparison in a condition.\n",
    "    #print(\"#4: \")\n",
    "    k=0; N=[np.array([e_sep])]; n=[1]; u = s-1; tried = np.array([0]*n_init); tried[e_sep] = 1\n",
    "    \n",
    "    seploop = 0\n",
    "    while True:\n",
    "        #first line:\n",
    "        #gamma_{k+1}(e):=get neighbours/set of points from e with the distance of k+1\n",
    "        N_next = np.where(distances == k+1)[0] #get the set of neighbours with distance = k+1\n",
    "        N.append(N_next)\n",
    "        n.append(len(N[k+1])) #or sum of weights?\n",
    "        u -= n[k+1]\n",
    "        #print(\"k,N,n,u\",k,N,n,u)\n",
    "\n",
    "        #print(\"n_arr[k] <= n_arr[k+1] < n_arr[k+2]\",n_arr[k] <= n_arr[k+1] < n_arr[k+2])\n",
    "        \n",
    "        #print(\"k+2, len(n_arr), d\",k+2, len(n_arr), d)\n",
    "        '''if k+2 < len(n_arr): #temporary fix, by skipping the block if k+2 >= len(n)\n",
    "            if (n_arr[k] <= n_arr[k+1] < n_arr[k+2]):\n",
    "                k += 1\n",
    "                continue'''\n",
    "        if (k < d-1) and (n_arr[k] <= n_arr[k+1] < n_arr[k+2]) and (u > 0.4*s): #another fix, by adding more skip-conditions\n",
    "            #print(\"(k < d-1) and (n_arr[k] <= n_arr[k+1] < n_arr[k+2]) and (u > 0.4*s) condition reached\")\n",
    "            k += 1\n",
    "            continue\n",
    "        \n",
    "        #second line, determining \"in degrees\":\n",
    "        #c = {} #need to know which c corresponds to which node, so probably use a dictionary\n",
    "        c = np.zeros(n_init)\n",
    "        j_idxs = N[k+1] #to keep track the used node indexes\n",
    "        for node_j in N[k+1]: #indexing not by nodes' indices, but by c's internal index\n",
    "            gamma_j = np.where(graph[node_j] == 1)[0]\n",
    "            c[node_j] = (np.intersect1d(gamma_j, N[k])).shape[0]\n",
    "            #print(\"gamma_j, N[k]\",gamma_j, N[k])\n",
    "        #print(\"c[j_idxs]\",c[j_idxs])\n",
    "\n",
    "        #third line, determining the \"out-weights\" (weights from normalization stage):\n",
    "        #b = {} #same reason with c\n",
    "        b = np.zeros(n_init)\n",
    "        i_idxs = N[k]\n",
    "        for node_i in N[k]:\n",
    "            gamma_i = np.where(graph[node_i] == 1)[0]\n",
    "            out_w_nodes = np.intersect1d(gamma_i, N[k+1])\n",
    "            b[node_i] = np.sum(w[out_w_nodes]) #w = weights from normalization, need to know which value belongs to which\n",
    "            #print(\"gamma_i, N[k+1]\",gamma_i, N[k+1])\n",
    "        #print(\"b\",b)\n",
    "        \n",
    "        \n",
    "        #fourth line:\n",
    "        while n[k] > 0:\n",
    "            #print(\"n[k]>0\",n[k] > 0)\n",
    "            if (u > 0.4*s) and (n[k+1] < n[k]): #threshold = 0.4s\n",
    "                #print(\"(u > 0.4*s) and (n[k+1] < n[k])\",(u > 0.4*s) and (n[k+1] < n[k]))\n",
    "                break\n",
    "            #place i with largest b_i last: (the rule should follow the placement rule in normalization)\n",
    "            #new condition to check, when b_i = 0, then break:\n",
    "            if np.sum(b) == 0:\n",
    "                #print(\"b_i all zero\")\n",
    "                #print(\"k,d,b,c\",k,d,b,c)\n",
    "                break\n",
    "            #\n",
    "            placed = np.argmax(b)\n",
    "            #print(\"placed\",placed)\n",
    "            ##start of temporary fix\n",
    "            #if b[placed] > 0: #meaning, gamma(i) \\intersect N_{k+1} is not {}\n",
    "            if w[placed] > 1:\n",
    "                ordered_list = get_ordered_list_merged_vertex(merge_forest, placed)\n",
    "                len_e = len(e)\n",
    "                len_ord_list = len(ordered_list)\n",
    "                e[len_e + last_zero - len_ord_list + 1 : len_e + last_zero + 1] = ordered_list\n",
    "                last_zero -= len_ord_list\n",
    "            else:\n",
    "                e[last_zero] = placed\n",
    "                last_zero -= 1\n",
    "            graph[placed] = graph[:,placed] = 0\n",
    "            deleted[placed] = True\n",
    "            b[placed] = 0 #remove from b\n",
    "            #print(\"e,fz,lz after placement:\",e,first_zero,last_zero)\n",
    "            #decrement s, n_k, c_j:\n",
    "            #print(\"s,n[k],c\",s,n[k],c)\n",
    "            s -= 1; n[k] -= 1; c[N[k+1]] -= 1\n",
    "            ##end of temporary fix#\n",
    "            #print(\"s,n[k],c\",s,n[k],c)\n",
    "            #if c_j == 0: ......\n",
    "            #drop c_j from N; incr u; decr n[k+1]:\n",
    "            for node_j in N[k+1]:\n",
    "                if c[node_j] == 0:\n",
    "                    N[k+1] = N[k+1][N[k+1] != node_j] #drop cj from N\n",
    "                    u += 1; n[k+1] -= 1\n",
    "                    #print(\"N, u, n, c[node_j]\",N, u, n, c[node_j])\n",
    "        if n[k] == 0: ##NOTE: this part is a little bit uncanny, since in first iter the n[k] will always reach 0\n",
    "            break\n",
    "        tried[N[k]] = 1; k += 1 #mark all i \\in N_k as tried, increment k\n",
    "        seploop+=1\n",
    "        #print(\"\\n seploop\",seploop)\n",
    "\n",
    "    #print(e)\n",
    "    #return graph, e, w, first_zero, last_zero, merge_forest\n",
    "    return first_zero, last_zero\n",
    "            #break #for loop breaking purpose during tests -- removed on actual scenario\n",
    "        #break #for loop breaking purpose during tests -- removed on actual scenario\n",
    "\n",
    "'''dummy data'''\n",
    "graph = np.array([\n",
    "            [0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 1, 0, 0, 1, 1, 1, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "            [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
    "            [1, 1, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "             ])\n",
    "\n",
    "#nauru graph (bipartite)\n",
    "graph_1 = np.zeros((24,24))\n",
    "graph_1[0][1] = graph_1[0][5] = graph_1[0][21] = \\\n",
    "graph_1[1][3] = graph_1[1][15] = \\\n",
    "graph_1[2][3] = graph_1[2][4] = graph_1[2][23] = \\\n",
    "graph_1[3][9] = graph_1[4][5] = graph_1[4][17] = \\\n",
    "graph_1[5][11] = graph_1[6][7] = graph_1[6][11] = graph_1[6][19] = \\\n",
    "graph_1[7][9] = graph_1[7][13] = \\\n",
    "graph_1[8][9] = graph_1[8][10] = graph_1[8][22] = \\\n",
    "graph_1[10][11] = graph_1[10][16] = \\\n",
    "graph_1[12][13] = graph_1[12][17] = graph_1[12][18] = \\\n",
    "graph_1[13][15] = graph_1[14][15] = graph_1[14][16] = graph_1[14][20] = \\\n",
    "graph_1[16][17] = graph_1[18][19] = graph_1[18][23] = \\\n",
    "graph_1[19][21] = graph_1[20][21] = graph_1[20][22] = \\\n",
    "graph_1[22][23] = 1\n",
    "graph_1 += graph_1.T\n",
    "#print(np.allclose(graph_1, graph_1.T, rtol=1e-05, atol=1e-08))\n",
    "graph = graph_1\n",
    "\n",
    "'''\n",
    "graph = np.array([[0,1,1,1,0,0,0],\n",
    "                    [1,0,1,1,1,0,0],\n",
    "                    [1,1,0,1,1,1,0],\n",
    "                    [1,1,1,0,1,1,1],\n",
    "                    [0,1,1,1,0,1,1],\n",
    "                    [0,0,1,1,1,0,1],\n",
    "                    [0,0,0,1,1,1,0]\n",
    "                   ])\n",
    "'''\n",
    "\n",
    "'''n_init = graph.shape[0]\n",
    "w = np.array([1]*n_init)\n",
    "first_zero = 0; last_zero = -1\n",
    "e = np.zeros(n_init)'''\n",
    "e,w,first_zero, last_zero, merge_forest, deleted = initialize(graph) \n",
    "'''end of dummy data'''\n",
    "\n",
    "separate(graph)\n",
    "print(\"e,w,first_zero, last_zero, merge_forest, deleted: \\n\",e,w,first_zero, last_zero, merge_forest, np.where(deleted == True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining both normalize and separate stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eli 1 [1 2 5 7 6 0 4 3]\n",
      "time elapsed for elimination ordering:  0.0019943714141845703 s\n",
      "metis 0 [2 1 0 5 3 7 4 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beryl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:200: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "'''input data: '''\n",
    "'''graph = np.array([\n",
    "            [0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 1, 0, 0, 1, 1, 1, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "            [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
    "            [1, 1, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "             ])'''\n",
    "#nauru graph (bipartite)\n",
    "graph_1 = np.zeros((24,24))\n",
    "graph_1[0][1] = graph_1[0][5] = graph_1[0][21] = \\\n",
    "graph_1[1][3] = graph_1[1][15] = \\\n",
    "graph_1[2][3] = graph_1[2][4] = graph_1[2][23] = \\\n",
    "graph_1[3][9] = graph_1[4][5] = graph_1[4][17] = \\\n",
    "graph_1[5][11] = graph_1[6][7] = graph_1[6][11] = graph_1[6][19] = \\\n",
    "graph_1[7][9] = graph_1[7][13] = \\\n",
    "graph_1[8][9] = graph_1[8][10] = graph_1[8][22] = \\\n",
    "graph_1[10][11] = graph_1[10][16] = \\\n",
    "graph_1[12][13] = graph_1[12][17] = graph_1[12][18] = \\\n",
    "graph_1[13][15] = graph_1[14][15] = graph_1[14][16] = graph_1[14][20] = \\\n",
    "graph_1[16][17] = graph_1[18][19] = graph_1[18][23] = \\\n",
    "graph_1[19][21] = graph_1[20][21] = graph_1[20][22] = \\\n",
    "graph_1[22][23] = 1\n",
    "graph_1 += graph_1.T\n",
    "#print(np.allclose(graph_1, graph_1.T, rtol=1e-05, atol=1e-08))\n",
    "\n",
    "#custom graph\n",
    "graph_2 = np.array([\n",
    "    [0,1,0,0,1,0,1,1],\n",
    "    [1,0,0,0,1,0,0,1],\n",
    "    [0,0,0,0,0,0,0,1],\n",
    "    [0,0,0,0,1,1,1,1],\n",
    "    [1,1,0,1,0,1,1,1],\n",
    "    [0,0,0,1,1,0,1,0],\n",
    "    [1,0,0,1,1,1,0,1],\n",
    "    [1,1,1,1,1,0,1,0]\n",
    "])\n",
    "\n",
    "def elimination_ordering(graph, log=False):\n",
    "    #alternate normalize and separate while the graph is not empty\n",
    "    i=0\n",
    "    while np.sum(graph) > 0:\n",
    "        if np.sum(graph) == 0:\n",
    "            break\n",
    "        if log:\n",
    "            print(\"Normalize:\")\n",
    "        normalize(graph)\n",
    "        if log:\n",
    "            print(\"e, w, first_zero, last_zero, deleted\", e, w, first_zero, last_zero, np.where(deleted == True))\n",
    "        if np.sum(graph) == 0:\n",
    "            break\n",
    "        if log:\n",
    "            print(\"\\n Separate:\")\n",
    "        separate(graph)\n",
    "        if log:\n",
    "            print(\"e, w, first_zero, last_zero, deleted \\n\", e, w, first_zero, last_zero, np.where(deleted == True))\n",
    "        #print(graph, merge_forest)\n",
    "        if log:\n",
    "            print(\"==================NEW ROUND======================= \\n\")\n",
    "        print(\"stage iteration:\",i)\n",
    "        i += 1\n",
    "    return e\n",
    "\n",
    "'''initialization'''\n",
    "graph = graph_2\n",
    "graph_elim = np.copy(graph)\n",
    "graph_elim2 = np.copy(graph_elim)\n",
    "e, w, first_zero, last_zero, merge_forest, deleted = initialize(graph) #must be on global scope\n",
    "\n",
    "'''ordered elimination'''\n",
    "start = time.time()\n",
    "elimination_ordering(graph)\n",
    "fills, _ = eliminate(graph_elim, e)\n",
    "print(\"eli\",fills,e)\n",
    "end = time.time()\n",
    "print(\"time elapsed for elimination ordering: \",end - start,\"s\")\n",
    "\n",
    "\n",
    "'''test using metis'''\n",
    "#adj_mat_to_metis_file(graph_elim2, \"custgraph.metisgraph\")\n",
    "#metis_order = iperm_to_orderlist(\"matrices/nauru_bipartite.metisgraph.iperm\")\n",
    "metis_order = iperm_to_orderlist(\"matrices/custgraph.metisgraph.iperm\")\n",
    "fills, _ = eliminate(graph_elim2, metis_order)\n",
    "print(\"metis\", fills, metis_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total fill: 3\n",
      "total fill: 0\n",
      "[7 1 3 0 2 6 5 4]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'matrices/bcsstk01.mtx.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3bb06356025a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miperm_to_orderlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"matrices/disconn_graph.metisgraph.iperm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_matrix_market\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"matrices/bcsstk01.mtx.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-3bb06356025a>\u001b[0m in \u001b[0;36mload_matrix_market\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;34m'''test using matrices from matrix market'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m#filename = \"matrices/bcsstm01.mtx.gz\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmminfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mMatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmmread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\io\\mmio.py\u001b[0m in \u001b[0;36mmminfo\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mEither\u001b[0m \u001b[1;34m'general'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'symmetric'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'skew-symmetric'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'hermitian'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \"\"\"\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mMMFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# -----------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\io\\mmio.py\u001b[0m in \u001b[0;36minfo\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \"\"\"\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose_it\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\io\\mmio.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(filespec, mode)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfilespec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m                     \u001b[1;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilespec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mfilespec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.bz2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                     \u001b[1;32mimport\u001b[0m \u001b[0mbz2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"write\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'matrices/bcsstk01.mtx.gz'"
     ]
    }
   ],
   "source": [
    "def eliminate(graph, elimination_order):\n",
    "    '''elimination function: eliminates the vertices based on the order resulted from elimination ordering algorithms\n",
    "    - takes in the vertices order from the any elimination ordering algorithms (e.g. METIS' nested dissection)\n",
    "    - fill will be added when the \"center\" of the vertex is eliminated, e.g., 1-2-3, eliminate 2, fill(1-3), fill_count+=1\n",
    "    - for now, assume the fill will be comb(n,2), so if there are 3 vertices which depend on an eliminated vertex, there will be 6 fills\n",
    "    '''\n",
    "    count_fill = 0\n",
    "    for node_i in elimination_order:\n",
    "        #find neighbours and fill the fill-in indexes:\n",
    "        neighbours = np.where(graph[node_i] == 1)[0]\n",
    "        fill_idxs = list(combinations(neighbours, 2))\n",
    "        #fill-in the edge of i's neighbours:\n",
    "        if len(fill_idxs) > 0:\n",
    "            #check if the edges are present, if there are no edges, add them:\n",
    "            for fill in fill_idxs:\n",
    "                if graph[fill[0]][fill[1]] == 0:\n",
    "                    graph[fill[0]][fill[1]] = graph[fill[1]][fill[0]] = 1\n",
    "                    count_fill += 1\n",
    "        #remove the edges of i:\n",
    "        graph[node_i] = graph[:,node_i] = 0\n",
    "    return count_fill, graph\n",
    "\n",
    "def adj_mat_to_metis_file(graph, filename):\n",
    "    '''write adjacency matrix to file'''\n",
    "    first_line = np.array([graph.shape[0], int(np.sum(graph)/2)]) #[nodes, edges]\n",
    "    adj_list = []\n",
    "    for i in range(graph.shape[0]):\n",
    "        neighbours = np.where(graph[i] == 1)[0]\n",
    "        neighbours += 1\n",
    "        adj_list.append(neighbours)\n",
    "    adj_list = np.array(adj_list)\n",
    "    \n",
    "    with open(filename,\"w\") as f:\n",
    "        f.write(str(first_line[0])+\" \"+str(first_line[1])+\"\\n\")\n",
    "        for i in range(adj_list.shape[0]):\n",
    "            for j in range(adj_list[i].shape[0]):\n",
    "                f.write(str(adj_list[i][j])+\" \")\n",
    "            f.write(\"\\n\")\n",
    "    print(\"writing\",filename,\"done!\")\n",
    "\n",
    "def iperm_to_orderlist(filename):\n",
    "    '''read iperm from ndmetis and convert it to list'''\n",
    "    f = open(filename, \"r\")\n",
    "    order = []\n",
    "    for x in f:\n",
    "        order.append(int(x))\n",
    "    order = np.array(order)\n",
    "    #according to metis documentatoin:\n",
    "    actual_order = np.zeros(order.shape[0])\n",
    "    for i in range(order.shape[0]):\n",
    "        actual_order[i] = np.where(order == i)[0]\n",
    "    actual_order = actual_order.astype(np.int64, copy=False)\n",
    "    return actual_order\n",
    "\n",
    "def load_matrix_market(filename):\n",
    "    '''test using matrices from matrix market'''\n",
    "    #filename = \"matrices/bcsstm01.mtx.gz\"\n",
    "    print(mminfo(filename))\n",
    "    Matrix = mmread(filename)\n",
    "    A = Matrix.toarray()\n",
    "    #print(A)\n",
    "    #print(np.nonzero(A))\n",
    "    '''preprocess the matrix'''\n",
    "    A = A.astype(np.int64, copy=False)\n",
    "    #symmetrize the matrix:\n",
    "    A = A + A.T\n",
    "    #print(\"symmetrize:\")\n",
    "    #print(A)\n",
    "    #set diagonals to zero:\n",
    "    np.fill_diagonal(A, 0)\n",
    "    #print(\"diag\")\n",
    "    #print(A)\n",
    "    #if a nonzero element is >0 or <0, set it to 1:\n",
    "    #print(\"nz\")\n",
    "    A[np.nonzero(A)] = 1\n",
    "    #print(A)\n",
    "    return A\n",
    "\n",
    "graph = np.array([\n",
    "            [0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 1, 0, 0, 1, 1, 1, 0],\n",
    "            [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "            [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
    "            [1, 1, 0, 1, 1, 0, 0, 0, 0],\n",
    "            [1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "             ])\n",
    "order = [2, 3, 4, 5, 6, 8, 1, 0, 7] #from the elimination ordering result\n",
    "countfill,_ = eliminate(graph, order)\n",
    "print(\"total fill:\" ,countfill)\n",
    "\n",
    "#banded matrix 2m+1 = 15\n",
    "graph_2 = np.array([[0,1,1,1,0,0,0],\n",
    "                    [1,0,1,1,1,0,0],\n",
    "                    [1,1,0,1,1,1,0],\n",
    "                    [1,1,1,0,1,1,1],\n",
    "                    [0,1,1,1,0,1,1],\n",
    "                    [0,0,1,1,1,0,1],\n",
    "                    [0,0,0,1,1,1,0]\n",
    "                   ])\n",
    "order = [0, 1, 2, 6, 5, 4, 3]\n",
    "countfill,_ = eliminate(graph, order)\n",
    "print(\"total fill:\" ,countfill)\n",
    "\n",
    "print(iperm_to_orderlist(\"matrices/disconn_graph.metisgraph.iperm\"))\n",
    "\n",
    "A = load_matrix_market(\"matrices/bcsstk01.mtx.gz\")\n",
    "print(np.nonzero(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bcsstk01.mtx.gz', 'bcsstm13.mtx.gz', 'mcca.mtx.gz', 'mcfe.mtx.gz']\n",
      "(48, 48, 224, 'coordinate', 'real', 'symmetric')\n",
      "True\n",
      "writing matrices/ndmetis_input/bcsstk01.mtx.gz.metisgraph done!\n",
      "(2003, 2003, 11973, 'coordinate', 'real', 'symmetric')\n",
      "True\n",
      "writing matrices/ndmetis_input/bcsstm13.mtx.gz.metisgraph done!\n",
      "(180, 180, 2659, 'coordinate', 'real', 'general')\n",
      "True\n",
      "writing matrices/ndmetis_input/mcca.mtx.gz.metisgraph done!\n",
      "(765, 765, 24382, 'coordinate', 'real', 'general')\n",
      "True\n",
      "writing matrices/ndmetis_input/mcfe.mtx.gz.metisgraph done!\n"
     ]
    }
   ],
   "source": [
    "'''matrices pre-processing'''\n",
    "mypath = \"matrices/mm/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "print(onlyfiles)\n",
    "#for all matrices, load:\n",
    "for f in onlyfiles:\n",
    "    G = load_matrix_market(\"matrices/mm/\"+f)\n",
    "    #print(np.diagonal(G))\n",
    "    print(np.allclose(G, G.T, rtol=1e-05, atol=1e-08))\n",
    "\n",
    "    #save all matrices to metis format:\n",
    "    adj_mat_to_metis_file(G, \"matrices/ndmetis_input/\"+f+\".metisgraph\")\n",
    "\n",
    "    #run the metis using all the matrices elsewhere.....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''testing main for matrix market'''\n",
    "#for all matrices, reload:\n",
    "mypath = \"matrices/mm/\"\n",
    "onlyfiles = np.array([f for f in listdir(mypath) if isfile(join(mypath, f))])\n",
    "#onlyfiles = onlyfiles[[0,2]] #only for testing subslices\n",
    "print(onlyfiles)\n",
    "times = []\n",
    "fills_eli = []\n",
    "fills_metis = []\n",
    "fills_eli_ratio = []\n",
    "fills_metis_ratio = []\n",
    "for f in onlyfiles:\n",
    "    G = load_matrix_market(mypath+f)\n",
    "    edges = np.sum(G[np.triu_indices(G.shape[0], 1)]) #sum of upper triangular\n",
    "    #initialize: MUST always be in global scope:\n",
    "    G1 = np.copy(G)\n",
    "    G2 = np.copy(G)\n",
    "    e, w, first_zero, last_zero, merge_forest, deleted = initialize(G)\n",
    "\n",
    "    #elimination ordering:\n",
    "    start = time.time()\n",
    "    elimination_ordering(G)\n",
    "    fills, _ = eliminate(G1, e)\n",
    "    fills_eli.append(fills)\n",
    "    print(\"eli\",fills,e)\n",
    "    #print(\"sorted e\", sorted(e))\n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    times.append(elapsed)\n",
    "    print(\"time elapsed for elimination ordering: \",elapsed,\"s\")\n",
    "    fills_eli_ratio.append(float(fills/edges))\n",
    "    \n",
    "    #metis:\n",
    "    metis_order = iperm_to_orderlist(\"matrices/ndmetis_iperm/\"+f+\".metisgraph.iperm\")\n",
    "    fills, _ = eliminate(G2, metis_order)\n",
    "    fills_metis.append(fills)\n",
    "    print(\"metis\", fills, metis_order)\n",
    "    fills_metis_ratio.append(float(fills/edges))\n",
    "    #print(\"sorted e\", sorted(metis_order))\n",
    "\n",
    "print(\"mm\",onlyfiles)\n",
    "print(\"eli\",fills_eli)\n",
    "print(\"metis\",fills_metis)\n",
    "print(\"eli fills/sum(triu)\",fills_eli_ratio)\n",
    "print(\"metis fills/sum(triu)\",fills_metis_ratio)\n",
    "print(\"eli_times\",times)\n",
    "#store results in file\n",
    "data = {\n",
    "    \"mm\": onlyfiles,\n",
    "    \"eli\": fills_eli,\n",
    "    \"eli_ratio\": fills_eli_ratio,\n",
    "    \"metis\": fills_metis,\n",
    "    \"metis_ratio\": fills_metis_ratio,\n",
    "    \"eli_times\": times,\n",
    "}\n",
    "with open('data.p', 'wb') as fp:\n",
    "    pickle.dump(data, fp)\n",
    "    \n",
    "with open('data.p', 'rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "    print(\"data = \",data)\n",
    "\n",
    "'''#check ordering result:\n",
    "e = np.array(e)\n",
    "e_sort = np.sort(e[e != -1])\n",
    "print(e_sort)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179]\n"
     ]
    }
   ],
   "source": [
    "'''above cell must be run first'''\n",
    "#e_sort = e_sort[2:]\n",
    "print(e_sort)\n",
    "#check missing:\n",
    "def printMissingElements(arr, N): \n",
    "    # Initialize diff \n",
    "    diff = arr[0] \n",
    "    for i in range(N): \n",
    "        # Check if diff and arr[i]-i \n",
    "        # both are equal or not \n",
    "        if(arr[i] - i != diff): \n",
    "            # Loop for consecutive \n",
    "            # missing elements \n",
    "            while(diff < arr[i] - i): \n",
    "                print(i + diff, end = \" \") \n",
    "                diff += 1\n",
    "printMissingElements(e_sort, e_sort.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 3, 7, 4], [2, 5, 6]]\n",
      "i, n, m, valency, valencies, e, modified, firstzero, lastzero\n",
      "\n",
      "0 8 2.75 3 [3 3 2 4 3 2 2 3] [-1 -1 -1 -1 -1 -1 -1 -1] [1 1 1 1 1 1 1 1] 8 0 -1\n",
      "neighbours [1 3 7]\n",
      "w,deleted [1 1 1 1 1 1 1 1] []\n",
      "\n",
      "1 8 2.75 3 [3 3 2 4 3 2 2 3] [-1 -1 -1 -1 -1 -1 -1 -1] [0 1 1 1 1 1 1 1] 7 0 -1\n",
      "neighbours [0 3 4]\n",
      "w,deleted [1 1 1 1 1 1 1 1] []\n",
      "\n",
      "2 8 2.75 2 [3 3 2 4 3 2 2 3] [-1 -1 -1 -1 -1 -1 -1 -1] [0 0 1 1 1 1 1 1] 6 0 -1\n",
      "neighbours [5 6]\n",
      "rule 4\n",
      "w,deleted [1 1 1 1 1 1 1 1] [2]\n",
      "\n",
      "3 7 2.5714285714285716 4 [3 3 0 4 3 1 1 3] [ 2 -1 -1 -1 -1 -1 -1 -1] [0 0 0 1 1 1 1 1] 5 1 -1\n",
      "neighbours [0 1 4 7]\n",
      "w,deleted [1 1 1 1 1 1 1 1] [2]\n",
      "\n",
      "4 7 2.5714285714285716 3 [3 3 0 4 3 1 1 3] [ 2 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 1 1 1 1] 4 1 -1\n",
      "neighbours [1 3 7]\n",
      "w,deleted [1 1 1 1 1 1 1 1] [2]\n",
      "\n",
      "5 7 2.5714285714285716 1 [3 3 0 4 3 1 1 3] [ 2 -1 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 1 1 1] 3 1 -1\n",
      "neighbours [6]\n",
      "rule 3\n",
      "w,deleted [1 1 1 1 1 1 1 1] [2 5]\n",
      "\n",
      "6 5 3.2 0 [3 3 0 4 3 0 0 3] [ 2  5 -1 -1 -1 -1 -1 -1] [0 0 0 0 0 0 1 1] 2 2 -1\n",
      "neighbours []\n",
      "rule 3\n",
      "w,deleted [1 1 1 1 1 1 1 1] [2 5 6]\n",
      "\n",
      "7 5 3.2 3 [3 3 0 4 3 0 0 3] [ 2  5  6 -1 -1 -1 -1 -1] [0 0 0 0 0 0 0 1] 1 3 -1\n",
      "neighbours [0 3 4]\n",
      "rule 6\n",
      "[0 3 4] [1 1 1]\n",
      "merged 7 3\n",
      "w,deleted [1 1 1 2 1 1 1 1] [2 5 6 7]\n",
      "\n",
      "\n",
      "0 4 2.5 2 [2 3 0 3 2 0 0 0] [ 2  5  6 -1 -1 -1 -1 -1] [1 0 0 1 1 0 0 0] 3 3 -1\n",
      "neighbours [1 3]\n",
      "rule 4\n",
      "w,deleted [1 1 1 2 1 1 1 1] [0 2 5 6 7]\n",
      "\n",
      "1 3 2.0 2 [0 2 0 2 2 0 0 0] [ 2  5  6  0 -1 -1 -1 -1] [0 1 0 1 1 0 0 0] 3 4 -1\n",
      "neighbours [3 4]\n",
      "rule 1\n",
      "w,deleted [1 1 1 2 1 1 1 1] [0 1 2 5 6 7]\n",
      "\n",
      "already deleted: 2\n",
      "3 2 1.0 1 [0 0 0 1 1 0 0 0] [ 2  5  6  0 -1 -1 -1  1] [0 0 0 1 1 0 0 0] 2 4 -2\n",
      "neighbours [4]\n",
      "S [7]\n",
      "rule 1\n",
      "w,deleted [1 1 1 2 1 1 1 1] [0 1 2 3 5 6 7]\n",
      "\n",
      "4 0 nan 0 [0 0 0 0 0 0 0 0] [ 2  5  6  0 -1  7  3  1] [0 0 0 0 1 0 0 0] 1 4 -4\n",
      "neighbours []\n",
      "rule 3\n",
      "w,deleted [1 1 1 2 1 1 1 1] [0 1 2 3 4 5 6 7]\n",
      "\n",
      "already deleted: 5\n",
      "already deleted: 6\n",
      "already deleted: 7\n",
      "[2 5 6 0 4 7 3 1]\n",
      "from ndmetis [7 1 3 0 2 6 5 4]\n",
      "fills 1\n",
      "from eli normalize [2 5 6 0 4 7 3 1]\n",
      "fills 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beryl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:195: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "'''disconnected graph checking, if it is assumed as different \\gamma'''\n",
    "G = np.array([\n",
    "        [0,0,0,1,1,0,0,0],\n",
    "        [0,0,1,0,0,0,0,0],\n",
    "        [0,1,0,0,0,0,0,0],\n",
    "        [1,0,0,0,1,0,0,0],\n",
    "        [1,0,0,1,0,0,0,0],\n",
    "        [0,0,0,0,0,0,1,1],\n",
    "        [0,0,0,0,0,1,0,1],\n",
    "        [0,0,0,0,0,1,1,0]\n",
    "    ])\n",
    "\n",
    "G = np.array([\n",
    "        [0,1,0,1,0,0,0,1],\n",
    "        [1,0,0,1,1,0,0,0],\n",
    "        [0,0,0,0,0,1,1,0],\n",
    "        [1,1,0,0,1,0,0,1],\n",
    "        [0,1,0,1,0,0,0,1],\n",
    "        [0,0,1,0,0,0,1,0],\n",
    "        [0,0,1,0,0,1,0,0],\n",
    "        [1,0,0,1,1,0,0,0]\n",
    "        \n",
    "    ])\n",
    "G_elim = np.copy(G)\n",
    "visited = np.array([False]*G.shape[0])\n",
    "traverse_count = 0\n",
    "sub_Gs = []\n",
    "for i in range(G.shape[0]):\n",
    "    if not visited[i]:\n",
    "        sub_G = BFS(G,i)\n",
    "        sub_Gs.append(sub_G)\n",
    "        visited[sub_G] = True\n",
    "print(sub_Gs)\n",
    "\n",
    "'''generating subgraphs from a whole matrix graph'''\n",
    "'''Gs = []\n",
    "for i in range(len(sub_Gs)):\n",
    "    #set all indices except i to 0\n",
    "    sliced = []\n",
    "    for j in range(len(sub_Gs)):\n",
    "        if j!=i:\n",
    "            sliced.append(sub_Gs[j])\n",
    "    print(sliced)\n",
    "    sliced = [item for sublist in sliced for item in sublist] #flattened list\n",
    "    print(sliced)\n",
    "    sub_graph = np.copy(G)\n",
    "    sub_graph[sliced] = sub_graph[:,sliced] = 0\n",
    "    Gs.append(sub_graph)\n",
    "print(Gs)\n",
    "for G in Gs:\n",
    "    G_elim = np.copy(G)\n",
    "    e,w,first_zero,last_zero,merge_forest,deleted = initialize(G)\n",
    "    #normalize(G)\n",
    "    separate(G)\n",
    "    print(\"e\",e)\n",
    "    print(eliminate(G_elim,e))\n",
    "    print()\n",
    "'''\n",
    "e,w,first_zero,last_zero,merge_forest,deleted = initialize(G_elim) #always initialize before normalize\n",
    "normalize(G)\n",
    "print(e)\n",
    "elim_order_metis = iperm_to_orderlist(\"disconn_graph.metisgraph.iperm\")\n",
    "print(\"from ndmetis\",elim_order_metis)\n",
    "G_elim2 = np.copy(G_elim)\n",
    "fills, _ = eliminate(G_elim, elim_order_metis) #using metis\n",
    "print(\"fills\",fills)\n",
    "fills, _ = eliminate(G_elim2, e) #using eli\n",
    "print(\"from eli normalize\", e)\n",
    "print(\"fills\",fills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "G = np.array([\n",
    "        [0,1,0,1,0,0,0,1],\n",
    "        [1,0,0,1,1,0,0,0],\n",
    "        [0,0,0,0,0,1,1,0],\n",
    "        [1,1,0,0,1,0,0,1],\n",
    "        [0,1,0,1,0,0,0,1],\n",
    "        [0,0,1,0,0,0,1,0],\n",
    "        [0,0,1,0,0,1,0,0],\n",
    "        [1,0,0,1,1,0,0,0]\n",
    "        \n",
    "    ])\n",
    "upper_tri = G[np.triu_indices(G.shape[0], 1)]\n",
    "#upper_tri = np.triu_indices(G.shape[0], 1)\n",
    "print(np.sum(upper_tri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
